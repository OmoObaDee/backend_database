{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85256e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b512ca9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['case_id', 'continent', 'education_of_employee', 'has_job_experience',\n",
      "       'requires_job_training', 'no_of_employees', 'yr_of_estab',\n",
      "       'region_of_employment', 'prevailing_wage', 'unit_of_wage',\n",
      "       'full_time_position', 'case_status'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset for Reprocessing\n",
    "\n",
    "cleaned_data_path=\"https://raw.githubusercontent.com/ek-chris/Practice_datasets/refs/heads/main/EasyVisa%20(1).csv\"\n",
    "\n",
    "df = pd.read_csv(cleaned_data_path)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ======================================================\n",
    "# # DATA REPROCESSING\n",
    "# # ======================================================\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# import json\n",
    "# import os\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# def reprocess_data(\n",
    "#     cleaned_data_path,\n",
    "#     target_column=pd.Index,\n",
    "#     output_dir=\"reprocessing_results\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Full preprocessing function for cleaned dataset.\n",
    "#     -------------------------------------------------------\n",
    "#     Parameters:\n",
    "#         cleaned_data_path (str): Path to cleaned dataset.\n",
    "#         target_column (str): The name of the target variable.\n",
    "#         output_dir (str): Folder to save processed files.\n",
    "#     -------------------------------------------------------\n",
    "#     Returns:\n",
    "#         X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
    "#     -------------------------------------------------------\n",
    "#     \"\"\"\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 1. Create Output Folder\n",
    "#     # ---------------------------\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     print(f\"Processed results will be saved to: {output_dir}\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 2. Load Cleaned Dataset\n",
    "#     # ---------------------------\n",
    "#     df = pd.read_csv(cleaned_data_path)\n",
    "#     print(f\"Cleaned Data Loaded â€” Shape: {df.shape}\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 3. Encode Categorical Columns\n",
    "#     # ---------------------------\n",
    "#     cat_cols = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "#     label_encoders = {}\n",
    "\n",
    "#     for col in cat_cols:\n",
    "#         le = LabelEncoder()\n",
    "#         df[col] = le.fit_transform(df[col].astype(str))\n",
    "#         label_encoders[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "#     print(f\"Encoded {len(cat_cols)} categorical columns.\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 4. Split Features & Target\n",
    "#     # ---------------------------\n",
    "#     X = df.drop(columns=[target_column])\n",
    "#     y = df[target_column]\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 5. Train / Validation / Test Split\n",
    "#     # ---------------------------\n",
    "#     X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "#         X, y, test_size=0.3, random_state=42, stratify=y if len(y.unique()) <= 10 else None\n",
    "#     )\n",
    "#     X_val, X_test, y_val, y_test = train_test_split(\n",
    "#         X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp if len(y.unique()) <= 10 else None\n",
    "#     )\n",
    "\n",
    "#     print(f\"Split completed:\")\n",
    "#     print(f\"   Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 6. Scale Numerical Features\n",
    "#     # ---------------------------\n",
    "#     num_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "#     scaler = StandardScaler()\n",
    "\n",
    "#     X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
    "#     X_val_scaled = pd.DataFrame(scaler.transform(X_val[num_cols]), columns=num_cols)\n",
    "#     X_test_scaled = pd.DataFrame(scaler.transform(X_test[num_cols]), columns=num_cols)\n",
    "\n",
    "#     # Replace numeric columns with scaled versions\n",
    "#     X_train_scaled = pd.concat([X_train_scaled, X_train.drop(columns=num_cols).reset_index(drop=True)], axis=1)\n",
    "#     X_val_scaled = pd.concat([X_val_scaled, X_val.drop(columns=num_cols).reset_index(drop=True)], axis=1)\n",
    "#     X_test_scaled = pd.concat([X_test_scaled, X_test.drop(columns=num_cols).reset_index(drop=True)], axis=1)\n",
    "\n",
    "#     print(\"Numeric columns scaled using StandardScaler.\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 7. Save Processed Files\n",
    "#     # ---------------------------\n",
    "#     X_train_scaled.to_csv(os.path.join(output_dir, \"X_train_scaled.csv\"), index=False)\n",
    "#     X_val_scaled.to_csv(os.path.join(output_dir, \"X_val_scaled.csv\"), index=False)\n",
    "#     X_test_scaled.to_csv(os.path.join(output_dir, \"X_test_scaled.csv\"), index=False)\n",
    "#     y_train.to_csv(os.path.join(output_dir, \"y_train.csv\"), index=False)\n",
    "#     y_val.to_csv(os.path.join(output_dir, \"y_val.csv\"), index=False)\n",
    "#     y_test.to_csv(os.path.join(output_dir, \"y_test.csv\"), index=False)\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 8. Save Summary Report\n",
    "#     # ---------------------------\n",
    "#     preprocessing_summary = {\n",
    "#         \"data_shape\": df.shape,\n",
    "#         \"categorical_columns_encoded\": cat_cols,\n",
    "#         \"numerical_columns_scaled\": num_cols,\n",
    "#         \"train_shape\": X_train.shape,\n",
    "#         \"validation_shape\": X_val.shape,\n",
    "#         \"test_shape\": X_test.shape,\n",
    "#         \"label_encoders\": label_encoders\n",
    "#     }\n",
    "\n",
    "#     with open(os.path.join(output_dir, \"preprocessing_summary.json\"), \"w\") as f:\n",
    "#         json.dump(preprocessing_summary, f, indent=2)\n",
    "\n",
    "#     print(\"Preprocessing summary saved as 'preprocessing_summary.json'\")\n",
    "\n",
    "#     print(\"\\n Reprocessing completed successfully â€” all files saved.\")\n",
    "#     return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# # Example Usage:\n",
    "# # X_train, X_val, X_test, y_train, y_val, y_test = reprocess_data(\n",
    "# #     cleaned_data_path=\"EasyVisa_EDA_Results/cleaned_easyvisa.csv\",\n",
    "# #     target_column=\"Loan_Status\",   # change to your actual target\n",
    "# #     output_dir=\"EasyVisa_Reprocessed\"\n",
    "# # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a685e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# def reprocess_data(cleaned_data_path, target_column=None, output_dir=\"reprocessing_results\"):\n",
    "#     \"\"\"\n",
    "#     Reprocess cleaned dataset for modeling.\n",
    "#     Includes handling missing data, encoding categorical features,\n",
    "#     scaling numeric features, splitting train/val/test sets, \n",
    "#     and saving processed files.\n",
    "\n",
    "#     Args:\n",
    "#         cleaned_data_path (str): Path to cleaned CSV file.\n",
    "#         target_column (str, optional): Name of the target column. \n",
    "#                                        If None, tries to infer automatically.\n",
    "#         output_dir (str): Directory to save reprocessed outputs.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: Containing processed DataFrames and file paths.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # ---------------------------\n",
    "#     # 1. Load Cleaned Data\n",
    "#     # ---------------------------\n",
    "#     df = pd.read_csv(cleaned_data_path)\n",
    "#     print(f\"âœ… Loaded cleaned dataset with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 2. Infer Target Column if Not Provided\n",
    "#     # ---------------------------\n",
    "#     if target_column is None:\n",
    "#         # Heuristic: look for 'target', 'class', or 'status' in column names\n",
    "#         possible_targets = [c for c in df.columns if c.lower() in ['target', 'class', 'status', 'loan_status', 'approved']]\n",
    "#         if possible_targets:\n",
    "#             target_column = possible_targets[0]\n",
    "#             print(f\"âš™ï¸ Automatically detected target column: {target_column}\")\n",
    "#         else:\n",
    "#             raise ValueError(\"âŒ Could not infer target column. Please specify it explicitly.\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 3. Handle Missing Values\n",
    "#     # ---------------------------\n",
    "#     df.fillna(df.median(numeric_only=True), inplace=True)\n",
    "#     df.fillna(df.mode().iloc[0], inplace=True)\n",
    "#     print(\"ðŸ§¹ Missing values filled (numeric: median, categorical: mode).\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 4. Encode Categorical Features\n",
    "#     # ---------------------------\n",
    "#     cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "#     label_encoders = {}\n",
    "#     for col in cat_cols:\n",
    "#         le = LabelEncoder()\n",
    "#         df[col] = le.fit_transform(df[col].astype(str))\n",
    "#         label_encoders[col] = le\n",
    "#     print(f\"ðŸ”¤ Encoded {len(cat_cols)} categorical columns: {list(cat_cols)}\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 5. Split Features and Target\n",
    "#     # ---------------------------\n",
    "#     if target_column not in df.columns:\n",
    "#         raise KeyError(f\"âŒ Target column '{target_column}' not found in dataset columns: {df.columns.tolist()}\")\n",
    "    \n",
    "#     X = df.drop(columns=[target_column])\n",
    "#     y = df[target_column]\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 6. Scale Numeric Features\n",
    "#     # ---------------------------\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "#     print(\"âš–ï¸ Features scaled using StandardScaler.\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 7. Train / Validation / Test Split\n",
    "#     # ---------------------------\n",
    "#     X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "#     X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "#     print(f\"ðŸ“Š Data split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 8. Save All Outputs\n",
    "#     # ---------------------------\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     X_train.to_csv(f\"{output_dir}/X_train_scaled.csv\", index=False)\n",
    "#     X_val.to_csv(f\"{output_dir}/X_val_scaled.csv\", index=False)\n",
    "#     X_test.to_csv(f\"{output_dir}/X_test_scaled.csv\", index=False)\n",
    "#     y_train.to_csv(f\"{output_dir}/y_train.csv\", index=False)\n",
    "#     y_val.to_csv(f\"{output_dir}/y_val.csv\", index=False)\n",
    "#     y_test.to_csv(f\"{output_dir}/y_test.csv\", index=False)\n",
    "\n",
    "#     # Save metadata (like column names)\n",
    "#     summary = {\n",
    "#         \"target_column\": target_column,\n",
    "#         \"categorical_columns\": list(cat_cols),\n",
    "#         \"scaler\": \"StandardScaler\",\n",
    "#         \"rows\": len(df)\n",
    "#     }\n",
    "#     pd.Series(summary).to_json(f\"{output_dir}/reprocessing_summary.json\", indent=2)\n",
    "#     print(f\"ðŸ’¾ Processed data and summary saved to: {output_dir}\")\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 9. Return all objects\n",
    "#     # ---------------------------\n",
    "#     return {\n",
    "#         \"X_train\": X_train,\n",
    "#         \"X_val\": X_val,\n",
    "#         \"X_test\": X_test,\n",
    "#         \"y_train\": y_train,\n",
    "#         \"y_val\": y_val,\n",
    "#         \"y_test\": y_test,\n",
    "#         \"target_column\": target_column\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c5b08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "def reprocess_data(df, target_column=None, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Function to reprocess data for ML modeling.\n",
    "    Handles missing values, encoding, scaling, and splitting.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input cleaned DataFrame\n",
    "        target_column (str): Name of the target variable (if any)\n",
    "        test_size (float): Proportion of test data\n",
    "        val_size (float): Proportion of validation data\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        If target_column is provided:\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test (pd.DataFrames/Series)\n",
    "        Otherwise:\n",
    "            X (processed DataFrame)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting data reprocessing...\")\n",
    "\n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Handle missing values\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "    df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])\n",
    "    print(\"Missing values handled.\")\n",
    "\n",
    "    # 2. Encode categorical features\n",
    "    le = LabelEncoder()\n",
    "    for col in cat_cols:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    print(f\"Encoded {len(cat_cols)} categorical columns.\")\n",
    "\n",
    "    # 3. Separate features and target\n",
    "    if target_column and target_column in df.columns:\n",
    "        X = df.drop(columns=[target_column])\n",
    "        y = df[target_column]\n",
    "        print(f\"Target column detected: {target_column}\")\n",
    "    else:\n",
    "        X = df\n",
    "        y = None\n",
    "        print(\"No target column provided â€” returning only features.\")\n",
    "\n",
    "    # 4. Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "    print(\"Numerical features scaled.\")\n",
    "\n",
    "    # 5. Split data\n",
    "    if y is not None:\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=(test_size + val_size), random_state=random_state\n",
    "        )\n",
    "\n",
    "        val_relative_size = val_size / (test_size + val_size)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_temp, y_temp, test_size=1 - val_relative_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        print(\"Data split into train/validation/test sets successfully.\")\n",
    "        print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    else:\n",
    "        print(f\"Processed dataset shape: {X.shape}\")\n",
    "        return X\n",
    "\n",
    "# display output of Reprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbb60c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data reprocessing...\n",
      "Missing values handled.\n",
      "Encoded 9 categorical columns.\n",
      "No target column provided â€” returning only features.\n",
      "Numerical features scaled.\n",
      "Processed dataset shape: (25480, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>continent</th>\n",
       "      <th>education_of_employee</th>\n",
       "      <th>has_job_experience</th>\n",
       "      <th>requires_job_training</th>\n",
       "      <th>no_of_employees</th>\n",
       "      <th>yr_of_estab</th>\n",
       "      <th>region_of_employment</th>\n",
       "      <th>prevailing_wage</th>\n",
       "      <th>unit_of_wage</th>\n",
       "      <th>full_time_position</th>\n",
       "      <th>case_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>0.651230</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.398537</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.142282</td>\n",
       "      <td>0.533211</td>\n",
       "      <td>2</td>\n",
       "      <td>0.169835</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.694984</td>\n",
       "      <td>0.674834</td>\n",
       "      <td>4</td>\n",
       "      <td>0.919079</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.243429</td>\n",
       "      <td>-1.945186</td>\n",
       "      <td>4</td>\n",
       "      <td>0.169994</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.200417</td>\n",
       "      <td>0.604022</td>\n",
       "      <td>3</td>\n",
       "      <td>1.428604</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25475</th>\n",
       "      <td>17204</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.134020</td>\n",
       "      <td>0.674834</td>\n",
       "      <td>3</td>\n",
       "      <td>0.049924</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25476</th>\n",
       "      <td>17205</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.104603</td>\n",
       "      <td>0.627626</td>\n",
       "      <td>2</td>\n",
       "      <td>3.876159</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25477</th>\n",
       "      <td>17206</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.198713</td>\n",
       "      <td>-1.638337</td>\n",
       "      <td>3</td>\n",
       "      <td>1.360280</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25478</th>\n",
       "      <td>17207</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.163875</td>\n",
       "      <td>-2.181224</td>\n",
       "      <td>4</td>\n",
       "      <td>0.221509</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25479</th>\n",
       "      <td>17209</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.108056</td>\n",
       "      <td>-0.458148</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.067763</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25480 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case_id  continent  education_of_employee  has_job_experience  \\\n",
       "0            0          1                      2                   0   \n",
       "1            1          1                      3                   1   \n",
       "2            2          1                      0                   0   \n",
       "3            3          1                      0                   0   \n",
       "4            4          0                      3                   1   \n",
       "...        ...        ...                    ...                 ...   \n",
       "25475    17204          1                      0                   1   \n",
       "25476    17205          1                      2                   1   \n",
       "25477    17206          1                      3                   1   \n",
       "25478    17207          1                      3                   1   \n",
       "25479    17209          1                      0                   1   \n",
       "\n",
       "       requires_job_training  no_of_employees  yr_of_estab  \\\n",
       "0                          0         0.386667     0.651230   \n",
       "1                          0        -0.142282     0.533211   \n",
       "2                          1         1.694984     0.674834   \n",
       "3                          0        -0.243429    -1.945186   \n",
       "4                          0        -0.200417     0.604022   \n",
       "...                      ...              ...          ...   \n",
       "25475                      1        -0.134020     0.674834   \n",
       "25476                      0        -0.104603     0.627626   \n",
       "25477                      0        -0.198713    -1.638337   \n",
       "25478                      1        -0.163875    -2.181224   \n",
       "25479                      0        -0.108056    -0.458148   \n",
       "\n",
       "       region_of_employment  prevailing_wage  unit_of_wage  \\\n",
       "0                         4        -1.398537             0   \n",
       "1                         2         0.169835             3   \n",
       "2                         4         0.919079             3   \n",
       "3                         4         0.169994             3   \n",
       "4                         3         1.428604             3   \n",
       "...                     ...              ...           ...   \n",
       "25475                     3         0.049924             3   \n",
       "25476                     2         3.876159             3   \n",
       "25477                     3         1.360280             3   \n",
       "25478                     4         0.221509             3   \n",
       "25479                     1        -0.067763             3   \n",
       "\n",
       "       full_time_position  case_status  \n",
       "0                       1            1  \n",
       "1                       1            0  \n",
       "2                       1            1  \n",
       "3                       1            1  \n",
       "4                       1            0  \n",
       "...                   ...          ...  \n",
       "25475                   1            0  \n",
       "25476                   1            0  \n",
       "25477                   0            0  \n",
       "25478                   1            0  \n",
       "25479                   1            0  \n",
       "\n",
       "[25480 rows x 12 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df582e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
